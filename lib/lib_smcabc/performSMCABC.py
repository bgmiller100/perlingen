# -*- coding: utf-8 -*-
"""
@author: Brodie Lawson et al. (see README), Benjamin Miller 

This function performs SMC-ABC with resampling by "an independent Metropolis-Hastings kernel" as laid out by:
Drovandi, C. C. and Pettitt, A. N. (2011). Estimation of Parameters for
Macroparasite Population Evolution using Approximate Bayesian Computation.
Biometrics, 67(1):225-233.

Sets of particles satisfying a set of intermediary "discrepancy cutoffs"
are generated by successive resampling and mutation steps in the vein of
traditional SMC. The function is designed to be modular, so accepts as
input the functions that will be used to simulate the model, and to
calculate the discrepancy with the target data. Further options may be
specified by the user within the body of the program.

Usage:   [part_thetas, part_vals] = performSMCABC(N_parts, f_simulate, f_summaries, f_discrep, target_data, params_mins, params_maxs, scale_param, desired_D, visualise, (f_visualise), (options) )
        print('worst tempering ESS possible: %.4f'%(ESS[np.argmax(np.abs(ESStarget-ESS))]))

INPUTS
-------
N_parts - The number of particles to generate. Corresponds to the
          expected number of outputs satisfying the discrepancy criteria
          at the end of the method

f_simulate - The function used to simulate the model. This should
             generate only one output (but a struct could be used to
             store varied information/multiple outputs). The input is a
             single vector of parameters

f_summaries - The function used to calculate the summary statistics for
              given data. Takes as input a single object, which is the
              output of f_simulate. The target data should also be
              specified in this format

f_discrepancy - The function used to calculate the discrepancy between
                two sets of metrics. As a basis this would be the
               Euclidean or Mahalanobis distance, but is supplied
                separately so that extra problem-specific modifications
                can be made (e.g. taking into account periodicity of
                angular measures)

target_data - The data, of equivalent type to that generated by
              f_simulate, that the SMC_ABC routine seeks to generate
             samples sufficiently close to

params_mins - The minimum values allowed for the parameters of the model

params_maxs - The maximum values allowed for the parameters of the model

scale_param - Vector of logicals of length equal to number of parameters.
              Specifies which parameters are 'scale' parameters that are
              to be transformed logarithmically so as to have a uniform
              prior.

desired_D - The discrepancy that the algorithm will attempt to achieve
            (typical D values depend on application, so this is
            externally specified by the user)

NOT IMPLEMENTED:
    visualise - A boolean flag specifying if the user wants to visualise the
                output during the course of the algorithm

    (f_visualise) - A function (supplied by user because likely
                    application-specific) that visualises the results.
                    Optional argument, only used if visualise flag is set.
                    Syntax must be
                    visFunc( params, outputs, summaries, discrepancies )

    (options) - A struct containing modifications to the SMC-ABC algorithm's
                default options/parameters. Information regarding these is
                contained within the file. User may set:

                options.resample_weighting ('equal', 'weighted')
                options.metric_weighting ('mahalanobis', 'variance', 'none')
                options.keep_fraction ([0,1])
                options.max_MCMC_steps (positive integer)
                options.verbose (0 - false, 1 - true)

                For optional outputs, (options) must come last (but can be supplied after
                'visualise' flag if it is set to zero)

OUTPUTS
--------

part_thetas - The locations in parameter space of the final particles

part_vals - The model outputs corresponding to each particle
"""
import time
import numpy as np
def runSim(k, part_params, Pt, Ot, f_simulate, f_summaries): #seed_nums
    part_output = np.asarray(f_simulate(part_params, Pt, Ot),dtype='f8')
    part_summary = np.asarray(f_summaries(part_output),dtype='f8')
    return k, part_output, part_summary

def MVN_move(k, N_moves, theta, output, summaries, D, Ctheta, theta_mins, theta_maxs, scale_param, target_D, Pt_pass, Ot_pass, f_simulate, f_summaries, f_discrep, burnin=0):
    # This function performs MCMC updates ('move steps' or 'mutations') for a
    # single particle, using a multivariate normal jumping distribution
  
    # Initialise variable 'moved' as false
    moved = 0
    # Perform the requested number of move steps
    mvcount=0
    brokeout = 0
    burnpass = 0.08
    movespast = max(12, N_moves*burnpass)
    for y in range(N_moves):
        #disabled  burn in currently 
        if burnin==1 and mvcount>=movespast and moved==1: #will only reduce runs if given a burnin label
            brokeout = 1
            break #handle move to high-probability region with breakout speed-up
            
            #sudden, rapid degeneracy when reducing R to R/2
            #new attempt - break only after establishing a high probability position 
        # Pick a new location from the multivariate normal centred around the
        # requested point
        prop_theta = np.random.multivariate_normal(theta, Ctheta)
        
        # Instant rejection if particle moves outside boundary, so only process
        # if new proposed location is inside prior space
        if all(prop_theta <= theta_maxs) & all(prop_theta >= theta_mins):
            
            mydtype='f8'#'f8'
            # Convert scaling parameters back into true values for simulation
            prop_params = np.copy(prop_theta).astype(mydtype)
            prop_params[scale_param] = np.exp(prop_params[scale_param]);
            #print(prop_params[scale_param])
            # Calculate the associated model output, and discrepancy
            #time1=time.time()
            prop_output = np.asarray(f_simulate(prop_params, Pt_pass, Ot_pass),dtype=mydtype)
            #time2=time.time()
            prop_summaries = np.asarray(f_summaries(prop_output),dtype=mydtype)
            #time3=time.time()
            prop_D = np.asarray(f_discrep(prop_summaries),dtype=mydtype)
            #time4=time.time()
            #print('SimTime:%.4f, SumTime:%.4f, DisTime:%.4f'%(time2-time1,time3-time2,time4-time3))
            # Now accept this move if it still satisfies the current discrepancy
            # constraint (ratio of jumping distribution probabilities is one)
            if prop_D <= target_D:
                # Update all values for this particle
                theta[:] = prop_theta
                output[:] = prop_output
                D = np.copy(prop_D)
                summaries[:] = prop_summaries
                # Switch 'moved' flag to true
                moved = 1
                mvcount+=1
                #break #escape as soon as good, to hasten
    return k, theta, output, summaries, D, moved, brokeout

def warmup(x):
    x = x * x
    return x

def main(N_parts, f_simulate, f_summaries, f_discrepancy, target_data, params_mins, params_maxs, scale_param, desired_D, mesh,max_time):
    
    #import numpy as np
    import lhsmdu
    from multiprocessing import Pool, cpu_count, get_context
    import time
    from functools import partial
    import psutil
    import time
    from tqdm import tqdm
    
    '''
    %%%%%%%%%%%%%%%% VARIABLE INPUT %%%%%%%%%%%%%%%%%%%%%%%%%
    '''
    
    #Default options (can be changed here or adjusted on the fly by supplying options argument)
    #resample_weighting = 'weighted';  
    
    # 'equal' - All kept particles are weighted equally for resampling steps
    # 'weighted' - Particles are weighted according to their discrepancy values when resampling (taking
    #              worst kept particle as three sigma in a normal distribution, and then normalising
    #              weights so they sum to one)
    
    #metric_weighting = 'variance';  
    # 'mahalanobis' - Mahalanobis distance is used for calculations of discrepancy
    # 'variance' - Metrics are scaled according to their variance, but covariances are ignored
    # 'none' - Euclidean distance is used
    # (All options still include rescaling of angle metrics with respect to eccentricity)
    
    keep_fraction = 0.5 #0.75;         
    # The proportion of particles to keep during each resample step (recommended ~0.5 as a starting value)
    if N_parts<100:
        max_MCMC_steps = 50
    else:
        max_MCMC_steps = 300
    # The maximum number of 'jiggle' steps applied to all particles in the attempt to find unique locations
    verbose = 1;                 
    #Output information about particle uniqueness and discrepancy targets
    
    
    '''
    %%%%%%%%%%%%%%%% INSTANTIATE%%%%%%%%%%%%%%%%%%%%%%%%%
    '''

    process_count = cpu_count()  
    print('cpus: %d / %d'%(process_count,cpu_count()))
    max_runtime = time.time()+max_time
    print('beginning pool...')
    start = time.time()
    ESStarget=0.8
   
    burnidx = 50 #disabled
    #5 #number of iterations to burn-in (use full R)

    #generate and warm up pool 
    pool = get_context("spawn").Pool(processes = process_count, maxtasksperchild=1)
    time.sleep(1)
    end = time.time()
    print('time elapsed: '+str(end-start))
    print('warming up...')
    start = time.time()
    pool.map(warmup, range(process_count), chunksize=1)
    time.sleep(1)
    end  = time.time()
    print('time elapsed: '+str(end-start))
    print('parent precent memory used: '+str(psutil.virtual_memory()[2]))

    # Calculate the summaries for the target data
    print('calculating target summaries...')
    start=time.time()
    target_summaries = np.asarray(f_summaries(target_data),dtype='f8')
    end=time.time()
    #print('calculated, time elapsed: '+str(end-start))
    # Calculate the ordinal location of the last particle kept for convenience
    worst_keep = int(np.rint(N_parts * keep_fraction)+1)
    # Read out the number of variables
    N_theta = len(params_mins)
    
    # Convert scaling parameters to log equivalents. The set of transformed
    # parameters is termed 'theta' in this code
    #print('scaling...')
    scale_param = scale_param==1
    theta_mins = np.copy(params_mins).astype('f8')
    theta_mins[scale_param] = np.log(theta_mins[scale_param])
    theta_maxs = np.copy(params_maxs).astype('f8')
    theta_maxs[scale_param] = np.log(theta_maxs[scale_param])
   
    # Initialise particles using Latin Hypercube Sampling
    #print('generating lhsmdu...')
    part_lhs = lhsmdu.sample(N_parts, N_theta)    
    part_thetas = theta_mins + np.multiply(part_lhs,(theta_maxs - theta_mins))
    
    # For simulation, convert scaling parameters back to true values
    part_params = np.copy(part_thetas).astype('f8')
    part_params[:,scale_param] = np.exp(part_params[:,scale_param])

    # Load seed data (generated by a separate file)
    #print('loading seed data...')
    seedinfo = np.load("seedinfo.npy",allow_pickle=True)
    Pt = np.asarray(seedinfo.item().get('permute_tables'),dtype='f8')
    Ot = np.asarray(seedinfo.item().get('offset_tables'),dtype='f8')
    #Count number of seeds created
    N_seeds = np.shape(Pt)[0]
    
    '''
    %%%%%%%%%%%%%%%% PARPOOL 1: FIRST MCMC STEP %%%%%%%%%%%%%%%%%%%%%%%%%
    '''

    # Generate the seed information for each particle, then simulate the model
    # using this seed information, storing both its output and the associated
    # summary statistics    
    
    #Create an individual simulator object for this praticle, using the
    # seed information
    print('creating savespace...')
    seed_nums = np.random.choice(a=range(N_seeds),size=N_parts)
    part_outputs = np.zeros((N_parts, mesh['Nx'], mesh['Ny'])).astype('f8')
    part_summaries = np.zeros((N_parts, 27)).astype('f8')

    # Run the simulator and store summaries
    time.sleep(5)
    print('creating partial...')
    #runSimLoop  = partial(runSim, part_params=part_params, Pt=Pt, Ot=Ot, f_simulate=f_simulate, f_summaries=f_summaries, seed_nums=seed_nums)

    iterable = []
    for k in range(N_parts):
        iterable.append((k,part_params[k,:], Pt[seed_nums[k],:,:], Ot[seed_nums[k],:,:], f_simulate, f_summaries))
    time.sleep(5)
    #runSim_lambda = lambda k: runSim(k,part_params[k,:],Pt[seed_nums[k],:,:],Ot[seed_nums[k],:,:],f_simulate,f_summaries)
    print('parent precent memory used: '+str(psutil.virtual_memory()[2]))

    print('beginning map...')
    start_loop = time.time()
    #results = pool.map(runSimLoop,range(N_parts), chunksize=1)
    #results = []
    #for i,k,part_out, part_sum in tqdm(enumerate(pool.imap_unordered(runSimLoop,range(N_parts),chunksize=int(N_parts/process_count)))):
        #results.append(res)
    chunksize = int(N_parts/process_count)
    if chunksize<1:
        chunksize=1
    results = pool.starmap_async(runSim,iterable,chunksize=chunksize)
    results_get = results.get()
    for r in results_get:
        k = r[0]
        part_outputs[k,:,:] = r[1]
        part_summaries[k,:]=r[2]
        #pbar.update()
    del iterable
    print('time elapsed: '+str(time.time()-start_loop))
    print('parent precent memory used: '+str(psutil.virtual_memory()[2])+'\n')
    
    '''
    %%%%%%%%%%%%%%%% WEIGHTING/RESAMPLING %%%%%%%%%%%%%%%%%%%%%%%%%
    '''
            
    # Calculate sample covariance matrix from the initial particles
    C_S = np.cov(np.transpose(part_summaries))
    # Use diagonals of this to extract only variances (no covariances)
    C_S = np.diag(np.diag(C_S))
    # Store the inverse, so that it only need be calculated once
    invC_S = np.linalg.inv(C_S)
    # Use discrepancy function with this inverse matrix included
    f_discrep = partial(f_discrepancy,target_metrics_old = target_summaries, invC = invC_S)
    # Calculate discrepancies for each particle based on this distance measure
    part_Ds = np.asarray(f_discrepancy(part_summaries, target_summaries, invC_S),dtype='f8')

    # Loop until stopping criteria is hit
    looping = 1
    testitcount = 1
    while looping: 
        print('Iteration count: %g' % (testitcount,))
        if testitcount > burnidx:
            burnin=1
        else:
            burnin=0

        # First, sort all the particles according to their discrepancy
        ordering = np.argsort(part_Ds) 
        part_Ds = part_Ds[ordering]
        part_outputs = part_outputs[ordering,:,:]
        part_thetas = part_thetas[ordering,:]
        part_summaries = part_summaries[ordering,:]
        
        # Now select the target discrepancy as the worst particle of the kept
        # fraction
        target_D = np.copy(part_Ds[worst_keep])
        
        # Select which particles, from those that are kept, to resample the
        # discarded particles onto.
        # Calculate weights for each particle. These are found by
        # assuming that the worst kept particle is three standard
        # deviations away from D = 0. Due to normalisation of weights,
        # which here takes place inside randsample, even if all
        # particles are far from D = 0 it will not be an issue.
        
        part_logweights = - 9 * (part_Ds[:worst_keep]**2) / part_Ds[worst_keep]**2
        part_logweights = part_logweights + part_logweights.max() 
  
        #Normalise so largest weight is 1 (0 on log scale)
        #part_weights = np.exp(part_logweights)
        #part_weights = part_weights / np.sum(part_weights)

        # Tempering and Normalizing
        #via https://arxiv.org/pdf/1805.03317.pdf section 2.2
        BetaSet = np.linspace(0, 1, num=100)
        part_weights = np.exp(part_logweights)
        weights = np.tile(part_weights, (len(BetaSet),1))
        weights = np.divide(np.power(weights,BetaSet[:,None]),np.sum(np.power(weights,BetaSet[:,None]), axis=1)[:,None])
        ESS = (1/np.sum(weights**2, axis=1))/(N_parts*keep_fraction)
        ESSind = np.argmin(np.abs(ESStarget-ESS))
        alpha = 20 #cooling rate for beta*(iteration/alpha)
        Beta = BetaSet[ESSind]*max(1,((testitcount-1)/alpha)) 
        part_weights = part_weights**Beta / np.sum(part_weights**Beta)
        print('Weight tempering exponent: %.3f'%(Beta))



        # Now select particles from the kept particles, according to
        # their weights, to decide which particles to copy ontio
        selection = np.random.choice(a=range(worst_keep), size=N_parts-worst_keep,replace=False,p=part_weights)
   
        # Now perform the particle copy
        part_thetas[worst_keep:,:] = part_thetas[selection,:]
        part_outputs[worst_keep:,:,:] = part_outputs[selection,:,:]
        part_summaries[worst_keep:,:] = part_summaries[selection,:]
        part_Ds[worst_keep:] = part_Ds[selection]
        accepted = np.zeros(np.shape(part_Ds))
        
        # Now the particles are 'jiggled' so that we return to having a set of
        # unique particles. This step can also be thought of as performing
        # local exploration, and is sometimes called 'mutation'. MCMC steps are
        # used to perform mutation, and an advantage of SMC approaches is that
        # the positions of all particles can be used to decide on a sensible
        # jumping distribution.
        
        # Create a weighted covariance matrix to use in a multivariate
        # normal jumping distribution. 2.38^2/N is the 'optimal' factor
        # under some assumptions
        Ctheta = (2.38**2 / N_theta) * np.cov(np.transpose(part_thetas))

        '''
        %%%%%%%%%%%%%%%% PARPOOL 2: LOOP MCMC STEP %%%%%%%%%%%%%%%%%%%%%%%%%
        '''
        
        #Create an individual simulator object for this praticle, using the    
    
        # Perform one MCMC step to evaluate how many are expected to be required
        
        #MVN_loop_set = partial(MVN_move, N_moves=1, part_thetas=part_thetas, part_outputs=part_outputs, part_summaries=part_summaries, part_Ds=part_Ds, Ctheta=Ctheta, theta_mins=theta_mins, theta_maxs=theta_maxs, scale_param=scale_param, target_D=target_D, Pt=Pt, Ot=Ot, f_simulate=f_simulate, f_summaries=f_summaries, f_discrep=f_discrep, seed_nums=seed_nums)
        iterable = []
        for k in range(worst_keep,N_parts):
            iterable.append((k,1,np.array(part_thetas[k,:]).ravel(), np.squeeze(part_outputs[k,:,:]), np.squeeze(part_summaries[k,:]), np.squeeze(part_Ds[k]), Ctheta,theta_mins, theta_maxs, scale_param, target_D, np.squeeze(Pt[seed_nums[k],:,:]), np.squeeze(Ot[seed_nums[k],:,:]), f_simulate, f_summaries, f_discrep, burnin))

        print('mvn1...')
        start = time.time()
        #for i,k, part_theta, part_output, part_summary, part_D, accept  in tqdm(enumerate(pool.imap_unordered(MVN_loop_set,range(worst_keep,N_parts)))): 
        chunksize=int((N_parts-worst_keep)/process_count)
        if chunksize<1:
            chunksize=1
        results = pool.starmap_async(MVN_move,iterable,chunksize=chunksize)
        for r in results.get():
            k = r[0]
            part_thetas[k,:] = r[1]
            part_outputs[k,:,:] = r[2]
            part_summaries[k,:] = r[3]
            part_Ds[k] = r[4]
            accepted[k] = r[5]
        del iterable
        print('mvn1 duration: '+str(time.time()-start)) 
        
        # Calculate the acceptance rate, and ensure that the case where
        # all or no particles are accepted does not result in a number
        # of MCMC steps that cannot be calculated
        est_accept_rate = np.mean(accepted)
        est_accept_rate = np.where(est_accept_rate!=0,est_accept_rate,1*10**(-6) )    
        est_accept_rate = np.where(est_accept_rate!=1,est_accept_rate,1-1*10**(-6) )
        
        # Calculate the expected number of MCMC steps required from the
        # estimated acceptance rate. The number of steps cannot go
        # above a user-specified value
        R = np.ceil( np.log(0.05) / np.log(1 - est_accept_rate) )
        R = np.min([R,max_MCMC_steps])
        print('MCMC steps: %d'%(R))
        # Perform the remaining MCMC steps
        
        #MVN_loop_set = partial(MVN_move, N_moves=int(R-1), part_thetas=part_thetas, part_outputs=part_outputs, part_summaries=part_summaries, part_Ds=part_Ds, Ctheta=Ctheta, theta_mins=theta_mins, theta_maxs=theta_maxs, scale_param=scale_param, target_D=target_D, Pt=Pt, Ot=Ot, f_simulate=f_simulate, f_summaries=f_summaries, f_discrep=f_discrep, seed_nums=seed_nums)
   
        #print('Accept rate: %g'%(est_accept_rate))
        #print('Running remaining MVN loops %d to %d '%(worst_keep,N_parts))
        #print('Creating %d particles'%(int(R-1)))
        
        iterable = []
        for k in range(worst_keep,N_parts):
            iterable.append((k,int(R-1), np.array(part_thetas[k,:]).ravel(), np.squeeze(part_outputs[k,:,:]), np.squeeze(part_summaries[k,:]), np.squeeze(part_Ds[k]), Ctheta, theta_mins, theta_maxs, scale_param, target_D, np.squeeze(Pt[seed_nums[k],:,:]), np.squeeze(Ot[seed_nums[k],:,:]), f_simulate, f_summaries, f_discrep,burnin))

        print('mvn2...')
        start = time.time()
        #for i,k, part_theta, part_output, part_summary, part_D, accept  in tqdm(enumerate(pool.imap_unordered(MVN_loop_set,range(worst_keep,N_parts)))): 
        chunksize = int((N_parts-worst_keep)/process_count)
        if chunksize<1:
            chunksize=1
        brokeout=0
        results = pool.starmap_async(MVN_move,iterable,chunksize=chunksize)
        for r in results.get():
            k = r[0]
            part_thetas[k,:] = r[1]
            part_outputs[k,:,:] = r[2]
            part_summaries[k,:] = r[3]
            part_Ds[k] = r[4]
            accepted[k] = r[5]
            brokeout += r[6]
        del iterable
        print('mvn2 duration: '+str(time.time()-start)) 
        brokepercent = 100*brokeout / (N_parts-worst_keep)
        print('Breakout = %.2f %%'%(brokepercent))
        # Count the number of unique particles 
        #unique_thetas = np.unique(part_thetas, axis=0)
        N_unique = np.shape(np.unique(part_thetas,axis=0))[0]
        
        # Output information on discrepancy and uniqueness if flag is set
        if verbose:
            print("Current discrepancy target is %g, number of unique particles is %g" % (target_D, N_unique))
        
        # Check to see if the desired discrepancy target has been reached, and
        # terminate the loop if so. The structure of the loop means that a
        # mutation step will happen after the final resample, so we expect a
        # full sample of N_parts particles all satisfying the desired
        # discrepancy constraint
        print("Discrepency differnce: %.2f\n" % (desired_D-target_D,))
        if target_D < desired_D:
            looping = 0
        
        #Also check to see if degeneracy is occurring - this happens when the
        # MCMC steps fail to find new locations and thus remain copies of
        # previous particles. This also terminates the loop when it becomes too
        # severe, but with additional output of a warning
        #print("Degeneracy differnce: %.2f\n" % (N_unique-N_parts))
    
        if N_unique <= (N_parts / 2):
            print("WARNING: SMC loop terminated due to particle degeneracy. Target not reached! \n")
            looping = 0;
        
        '''
        # Visualise Particles if flag is set
        if visualise
            
            % Call provided plotting function
            f_visualise(part_thetas, part_outputs, part_summaries, part_Ds);
            drawnow;
            
        end
        '''
        '''
        #Memory and runtime safety features 
        mem_used = psutil.virtual_memory()[2]
        print('memory % used:', mem_used)
        if mem_used > 90:
            print('WARNING: High memory usage.  Aborting!\n')
            looping = 0
        if time.time()>max_runtime:
            #print('WARNING: Runtime exceeded.  Aborting!')
            looping = 0
        testitcount+=1
        '''
        break
    # Close the parallel pool now that it's use is finished
    pool.close()
    pool.join()
    del pool

    #Sort final unique output
    ordering = np.argsort(part_Ds) 
    part_thetas = np.copy(np.unique(part_thetas[ordering,:], axis=0))
    _,idx = np.unique(part_thetas,axis=0,return_index=True)
    
    part_thetas = np.copy(part_thetas[idx,:])
    part_Ds = np.copy(part_Ds[idx])
    part_outputs = np.copy(np.asarray(part_outputs[idx,:,:],dtype='f8'))
    part_summaries = np.copy(part_summaries[idx,:])

    print("Best discrepancy: %.2f, Worst discrepancy: %.2f"%(part_Ds[0], part_Ds[-1]))

    print('SMCABC COMPLETE\n')
    #print only unique output
    return part_thetas, part_outputs, part_summaries, part_Ds


if __name__=='__main__':
    pass
