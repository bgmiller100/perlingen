# -*- coding: utf-8 -*-
"""
Created on Wed Dec 18 12:53:17 2019

@author: Ben
"""
"""
% This function performs SMC-ABC as laid out by:
% Drovandi, C. C. and Pettitt, A. N. (2011). Estimation of Parameters for
% Macroparasite Population Evolution using Approximate Bayesian Computation.
% Biometrics, 67(1):225-233.
%
% Sets of particles satisfying a set of intermediary "discrepancy cutoffs"
% are generated by successive resampling and mutation steps in the vein of
% traditional SMC. The function is designed to be modular, so accepts as
% input the functions that will be used to simulate the model, and to
% calculate the discrepancy with the target data. Further options may be
% specified by the user within the body of the program.
%
%
% Usage:   [part_thetas, part_vals] = performSMCABC(N_parts, f_simulate, f_summaries, f_discrep, target_data, params_mins, params_maxs, scale_param, desired_D, visualise, (f_visualise), (options) )
%
% INPUTS
% -------
%
% N_parts - The number of particles to generate. Corresponds to the
%           expected number of outputs satisfying the discrepancy criteria
%           at the end of the method
%
% f_simulate - The function used to simulate the model. This should
%              generate only one output (but a struct could be used to
%              store varied information/multiple outputs). The input is a
%              single vector of parameters
%
% f_summaries - The function used to calculate the summary statistics for
%               given data. Takes as input a single object, which is the
%               output of f_simulate. The target data should also be
%               specified in this format
%
% f_discrepancy - The function used to calculate the discrepancy between
%                 two sets of metrics. As a basis this would be the
%                 Euclidean or Mahalanobis distance, but is supplied
%                 separately so that extra problem-specific modifications
%                 can be made (e.g. taking into account periodicity of
%                 angular measures)
%
% target_data - The data, of equivalent type to that generated by
%               f_simulate, that the SMC_ABC routine seeks to generate
%               samples sufficiently close to
%
% params_mins - The minimum values allowed for the parameters of the model
%
% params_maxs - The maximum values allowed for the parameters of the model
%
% scale_param - Vector of logicals of length equal to number of parameters.
%               Specifies which parameters are 'scale' parameters that are
%               to be transformed logarithmically so as to have a uniform
%               prior.
%
% desired_D - The discrepancy that the algorithm will attempt to achieve
%             (typical D values depend on application, so this is
%             externally specified by the user)
%
% visualise - A boolean flag specifying if the user wants to visualise the
%             output during the course of the algorithm
%
% (f_visualise) - A function (supplied by user because likely
%                 application-specific) that visualises the results.
%                 Optional argument, only used if visualise flag is set.
%                 Syntax must be
%                    visFunc( params, outputs, summaries, discrepancies )
%
% (options) - A struct containing modifications to the SMC-ABC algorithm's
%             default options/parameters. Information regarding these is
%             contained within the file. User may set:
%
%             options.resample_weighting ('equal', 'weighted')
%             options.metric_weighting ('mahalanobis', 'variance', 'none')
%             options.keep_fraction ([0,1])
%             options.max_MCMC_steps (positive integer)
%             options.verbose (0 - false, 1 - true)
%
% For optional outputs, (options) must come last (but can be supplied after
% 'visualise' flag if it is set to zero)%
% OUTPUTS
% --------
%
% part_thetas - The locations in parameter space of the final particles
%
% part_vals - The model outputs corresponding to each particle
"""

import numpy as np
def runSim(k, part_params, Pt, Ot, f_simulate, f_summaries): #seed_nums
    #print('memory precent used: '+str(psutil.virtual_memory()[2]))
    #locstart = time.time()
    #part_params_pass = part_params[k,:]
    #Pt_pass = Pt[seed_nums[k],:,:]
    #Ot_pass = Ot[seed_nums[k],:,:]
    part_output = f_simulate(part_params, Pt, Ot)
    #print("out fin\n")
    part_summary = f_summaries(part_output)
    #print("sum fin\n")
    #locend = time.time()
    #print('time for child: '+str(locend-locstart))
    return k, part_output, part_summary

def MVN_move(k, N_moves, theta, output, summaries, D, Ctheta, theta_mins, theta_maxs, scale_param, target_D, Pt_pass, Ot_pass, f_simulate, f_summaries, f_discrep):
    # This function performs MCMC updates ('move steps' or 'mutations') for a
    # single particle, using a multivariate normal jumping distribution
    #theta = np.array(part_thetas[k,:]).ravel()
    #output = np.squeeze(part_outputs[k,:,:])
    #summaries = np.squeeze(part_summaries[k,:])
    #D = np.squeeze(part_Ds[k])
    #Pt_pass = np.squeeze(Pt[seed_nums[k],:,:])
    #Ot_pass = np.squeeze(Ot[seed_nums[k],:,:])


    # Initialise variable 'moved' as false
    moved = 0
    # Perform the requested number of move steps
    
    for y in range(N_moves):
        # Pick a new location from the multivariate normal centred around the
        # requested point
        #print(np.shape(theta))
        #print(np.shape(summaries))
        #print(np.shape(Ctheta))
        prop_theta = np.random.multivariate_normal(theta, Ctheta)
        
        # Instant rejection if particle moves outside boundary, so only process
        # if new proposed location is inside prior space
        if all(prop_theta <= theta_maxs) & all(prop_theta >= theta_mins):
            
            # Convert scaling parameters back into true values for simulation
            prop_params = np.copy(prop_theta)
            prop_params[scale_param] = np.exp(prop_params[scale_param]);
            #print(prop_params[scale_param])
            # Calculate the associated model output, and discrepancy
            #print('sim...')
            prop_output = f_simulate(prop_params, Pt_pass, Ot_pass)
            #print('sum...')
            prop_summaries = f_summaries(prop_output)
            #print('disc...')
            prop_D = f_discrep(prop_summaries)
            #print('completes')         
            # Now accept this move if it still satisfies the current discrepancy
            # constraint (ratio of jumping distribution probabilities is one)
            if prop_D <= target_D:
                
                # Update all values for this particle
                theta[:] = prop_theta
                output[:] = prop_output
                D = np.copy(prop_D)
                summaries[:] = prop_summaries
                # Switch 'moved' flag to true
                moved = 1
        #else:
            #prop_D = np.inf
    #if moved==0:
        #print("MVN not occuring!!! target-prop= %.2f"%(target_D-prop_D))
    return k, theta, output, summaries, D, moved

def warmup(x):
    x = x * x
    return x

def main(N_parts, f_simulate, f_summaries, f_discrepancy, target_data, params_mins, params_maxs, scale_param, desired_D, mesh):
    
    #import numpy as np
    import lhsmdu
    from multiprocessing import Pool, cpu_count, get_context
    import time
    from functools import partial
    import psutil
    import time
    from tqdm import tqdm
    
    '''
    %%%%%%%%%%%%%%%% VARIABLE INPUT %%%%%%%%%%%%%%%%%%%%%%%%%
    '''
    
    #Default options (can be changed here or adjusted on the fly by supplying options argument)
    #resample_weighting = 'weighted';  
    
    # 'equal' - All kept particles are weighted equally for resampling steps
    # 'weighted' - Particles are weighted according to their discrepancy values when resampling (taking
    #              worst kept particle as three sigma in a normal distribution, and then normalising
    #              weights so they sum to one)
    
    #metric_weighting = 'variance';  
    # 'mahalanobis' - Mahalanobis distance is used for calculations of discrepancy
    # 'variance' - Metrics are scaled according to their variance, but covariances are ignored
    # 'none' - Euclidean distance is used
    # (All options still include rescaling of angle metrics with respect to eccentricity)
    
    keep_fraction = 0.5 #0.75;         
    # The proportion of particles to keep during each resample step (recommended ~0.5 as a starting value)
    if N_parts<100:
        max_MCMC_steps = 50
    else:
        max_MCMC_steps = 300
    # The maximum number of 'jiggle' steps applied to all particles in the attempt to find unique locations
    verbose = 1;                 
    #Output information about particle uniqueness and discrepancy targets
    
    
    '''
    %%%%%%%%%%%%%%%% INSTANTIATE%%%%%%%%%%%%%%%%%%%%%%%%%
    '''
    #print('cpus: '+str(cpu_count()))    
    process_count = cpu_count()  #replacing cpu_count() directly    
    print('cpus: %d / %d'%(process_count,cpu_count()))
    max_time = 300
    max_runtime = time.time()+max_time
    print('beginning pool...')
    start = time.time()
    #test maintaining this
    pool = get_context("spawn").Pool(processes = process_count, maxtasksperchild=1)
    time.sleep(1)
    end = time.time()
    print('time elapsed: '+str(end-start))
  
    print('warming up...')
    start = time.time()
    pool.map(warmup, range(process_count), chunksize=1)
    time.sleep(1)
    end  = time.time()
    print('time elapsed: '+str(end-start))
    
    print('parent precent memory used: '+str(psutil.virtual_memory()[2]))

    # Calculate the summaries for the target data
    print('calculating target summaries...')
    start=time.time()
    target_summaries = f_summaries(target_data)
    end=time.time()
    print('calculated, time elapsed: '+str(end-start))
    # Calculate the ordinal location of the last particle kept for convenience
    worst_keep = int(np.rint(N_parts * keep_fraction)+1)
    # Read out the number of variables
    N_theta = len(params_mins)
    
    # Convert scaling parameters to log equivalents. The set of transformed
    # parameters is termed 'theta' in this code
    #print('scaling...')
    scale_param = scale_param==1
    theta_mins = np.copy(params_mins)
    theta_mins[scale_param] = np.log(theta_mins[scale_param])
    theta_maxs = np.copy(params_maxs)
    theta_maxs[scale_param] = np.log(theta_maxs[scale_param])
   
    # Initialise particles using Latin Hypercube Sampling
    #print('generating lhsmdu...')
    part_lhs = lhsmdu.sample(N_parts, N_theta)    
    part_thetas = theta_mins + np.multiply(part_lhs,(theta_maxs - theta_mins))
    
    # For simulation, convert scaling parameters back to true values
    part_params = np.copy(part_thetas)
    part_params[:,scale_param] = np.exp(part_params[:,scale_param])

    # Load seed data (generated by a separate file)
    #print('loading seed data...')
    seedinfo = np.load("seedinfo.npy",allow_pickle=True)
    Pt = seedinfo.item().get('permute_tables')
    Ot = seedinfo.item().get('offset_tables')
    #Count number of seeds created
    N_seeds = np.shape(Pt)[0]
    
    '''
    %%%%%%%%%%%%%%%% PARPOOL 1: FIRST MCMC STEP %%%%%%%%%%%%%%%%%%%%%%%%%
    '''

    # Generate the seed information for each particle, then simulate the model
    # using this seed information, storing both its output and the associated
    # summary statistics    
    #kList = range(0,N_parts)
    
    #Create an individual simulator object for this praticle, using the
    # seed information
    #seed_num_list = np.mod(kList, N_seeds*np.ones(np.shape(kList))).astype(int)
    #seed_nums = random.sample(range(N_seeds),N_parts)
    print('creating savespace...')
    seed_nums = np.random.choice(a=range(N_seeds),size=N_parts)
    part_outputs = np.zeros((N_parts, mesh['Nx'], mesh['Ny']))
    part_summaries = np.zeros((N_parts, 27))

    # Run the simulator and store summaries
    #print('Running initialization loop to '+str(N_parts))
    time.sleep(5)
    print('creating partial...')
    #runSimLoop  = partial(runSim, part_params=part_params, Pt=Pt, Ot=Ot, f_simulate=f_simulate, f_summaries=f_summaries, seed_nums=seed_nums)

    iterable = []
    for k in range(N_parts):
        iterable.append((k,part_params[k,:], Pt[seed_nums[k],:,:], Ot[seed_nums[k],:,:], f_simulate, f_summaries))
    time.sleep(5)
    #runSim_lambda = lambda k: runSim(k,part_params[k,:],Pt[seed_nums[k],:,:],Ot[seed_nums[k],:,:],f_simulate,f_summaries)
    
    print('parent precent memory used: '+str(psutil.virtual_memory()[2]))

    print('beginning map...')
    #pbar = tqdm(total=N_parts)
    start_loop = time.time()
    #results = pool.map(runSimLoop,range(N_parts), chunksize=1)
    #results = []
    #for i,k,part_out, part_sum in tqdm(enumerate(pool.imap_unordered(runSimLoop,range(N_parts),chunksize=int(N_parts/process_count)))):
        #results.append(res)
    chunksize = int(N_parts/process_count)
    if chunksize<1:
        chunksize=1
    results = pool.starmap_async(runSim,iterable,chunksize=chunksize)
    #results = map(runSim_lambda,range(N_parts))#iterable)
    #print('results aquired!')
    
    #time.sleep(1)
    #print('time elapsed: '+str(time.time()-start_loop))

    results_get = results.get()
    #results_get = list(results)
    #print(len(results_get))
    #print(len(results_get[0]))
    #tester = results_get[0]
    #print(len(results))#_get))
    #print(np.shape((tester[1])))
    for r in results_get:
        k = r[0]
        part_outputs[k,:,:] = r[1]
        part_summaries[k,:]=r[2]
        #pbar.update()
    del iterable
    print('time elapsed: '+str(time.time()-start_loop))
    #pbar.close()
    #testing not closing
    #pool.close() 
    #pool.join()
    #del pool
    #pool = Pool()
    #for k, part_output, part_summary in results:
        #print(part_summary)
        #part_outputs[k,:,:] = part_output
        #part_summaries[k,:] = part_summary    
    #print(part_outputs)
    #print(part_summaries)
    #compare with matlab to confirm functionality of ellipse code
    #print('Complete\n')
    print('parent precent memory used: '+str(psutil.virtual_memory()[2]))
    
    '''
    %%%%%%%%%%%%%%%% WEIGHTING/RESAMPLING %%%%%%%%%%%%%%%%%%%%%%%%%
    '''
            
    # Calculate sample covariance matrix from the initial particles
    C_S = np.cov(np.transpose(part_summaries))
    # Use diagonals of this to extract only variances (no covariances)
    C_S = np.diag(np.diag(C_S))
    # Store the inverse, so that it only need be calculated once
    invC_S = np.linalg.inv(C_S)
    #print(invC_S)
    # Use discrepancy function with this inverse matrix included
    f_discrep = partial(f_discrepancy,target_metrics_old = target_summaries, invC = invC_S)
    # Calculate discrepancies for each particle based on this distance measure
    part_Ds = f_discrepancy(part_summaries, target_summaries, invC_S)
    #print(part_Ds)
    # Loop until stopping criteria is hit

    looping = 1
    testitcount = 1
    while looping: 
        #print(part_Ds)
        print('Iteration count: %g' % (testitcount,))
        testitcount+=1
        # First, sort all the particles according to their discrepancy
        ordering = np.argsort(part_Ds) 
        part_Ds = part_Ds[ordering]
        part_outputs = part_outputs[ordering,:,:]
        part_thetas = part_thetas[ordering,:]
        part_summaries = part_summaries[ordering,:]
        
        # Now select the target discrepancy as the worst particle of the kept
        # fraction
        target_D = np.copy(part_Ds[worst_keep])
        
        # Select which particles, from those that are kept, to resample the
        # discarded particles onto.
        # Calculate weights for each particle. These are found by
        # assuming that the worst kept particle is three standard
        # deviations away from D = 0. Due to normalisation of weights,
        # which here takes place inside randsample, even if all
        # particles are far from D = 0 it will not be an issue.
        
        part_logweights = - 9 * (part_Ds[:worst_keep]**2) / part_Ds[worst_keep]**2
        part_logweights = part_logweights + part_logweights.max() 
  
        #Normalise so largest weight is 1 (0 on log scale)
        #print('max weight '+str(np.max(part_logweights)))
        part_weights = np.exp(part_logweights)
        part_weights = part_weights / np.sum(part_weights)

        # Now select particles from the kept particles, according to
        # their weights, to decide which particles to copy ontio
        #print(np.shape(range(worst_keep)))
        #print(N_parts-worst_keep)
        selection = np.random.choice(a=range(worst_keep), size=N_parts-worst_keep,replace=False)#,p=part_weights)
   
        # Now perform the particle copy
        part_thetas[worst_keep:,:] = part_thetas[selection,:]
        part_outputs[worst_keep:,:,:] = part_outputs[selection,:,:]
        part_summaries[worst_keep:,:] = part_summaries[selection,:]
        part_Ds[worst_keep:] = part_Ds[selection]
        accepted = np.zeros(np.shape(part_Ds))
        
        # Now the particles are 'jiggled' so that we return to having a set of
        # unique particles. This step can also be thought of as performing
        # local exploration, and is sometimes called 'mutation'. MCMC steps are
        # used to perform mutation, and an advantage of SMC approaches is that
        # the positions of all particles can be used to decide on a sensible
        # jumping distribution.
        
        # Create a weighted covariance matrix to use in a multivariate
        # normal jumping distribution. 2.38^2/N is the 'optimal' factor
        # under some assumptions
        Ctheta = (2.38**2 / N_theta) * np.cov(np.transpose(part_thetas))
        #Ctheta = np.transpose(Ctheta)
        #print(Ctheta)
        '''
        %%%%%%%%%%%%%%%% PARPOOL 2: LOOP MCMC STEP %%%%%%%%%%%%%%%%%%%%%%%%%
        '''
        
        #Create an individual simulator object for this praticle, using the    
    
        # Perform one MCMC step to evaluate how many are expected to behttps://www.google.com/webhp?hl=en&sa=X&ved=0ahUKEwjTotrk6NPmAhWbWc0KHVBVDroQPAgH
        # required
        
        #MVN_loop_set = partial(MVN_move, N_moves=1, part_thetas=part_thetas, part_outputs=part_outputs, part_summaries=part_summaries, part_Ds=part_Ds, Ctheta=Ctheta, theta_mins=theta_mins, theta_maxs=theta_maxs, scale_param=scale_param, target_D=target_D, Pt=Pt, Ot=Ot, f_simulate=f_simulate, f_summaries=f_summaries, f_discrep=f_discrep, seed_nums=seed_nums)
        iterable = []
        for k in range(worst_keep,N_parts):
            iterable.append((k,1,np.array(part_thetas[k,:]).ravel(), np.squeeze(part_outputs[k,:,:]), np.squeeze(part_summaries[k,:]), np.squeeze(part_Ds[k]), Ctheta,theta_mins, theta_maxs, scale_param, target_D, np.squeeze(Pt[seed_nums[k],:,:]), np.squeeze(Ot[seed_nums[k],:,:]), f_simulate, f_summaries, f_discrep))

        #print('Running first MVN loop %d to %d'%(worst_keep,N_parts))
        #pool = get_context("spawn").Pool()
        #pbar = tqdm(total=N_parts)
        print('mvn1...')
        start = time.time()
        #results = pool.map(MVN_loop_set, range(worst_keep,N_parts))
        #for i,k, part_theta, part_output, part_summary, part_D, accept  in tqdm(enumerate(pool.imap_unordered(MVN_loop_set,range(worst_keep,N_parts)))): 
        chunksize=int((N_parts-worst_keep)/process_count)
        if chunksize<1:
            chunksize=1
        results = pool.starmap_async(MVN_move,iterable,chunksize=chunksize)
        for r in results.get():
            k = r[0]
            part_thetas[k,:] = r[1]
            part_outputs[k,:,:] = r[2]
            part_summaries[k,:] = r[3]
            part_Ds[k] = r[4]
            accepted[k] = r[5]
            #pbar.update()
        #pbar.close()
        del iterable
        print('mvn1 duration: '+str(time.time()-start)) 
        #time.sleep(1)
        #end = time.time()
        #print('mvn1 duration: '+str(end-start))
        #time.sleep(1) 
        #pool.close() 
        #pool.join()
        #del pool

        #print('Complete\n')
        
        # Calculate the acceptance rate, and ensure that the case where
        # all or no particles are accepted does not result in a number
        # of MCMC steps that cannot be calculated
        est_accept_rate = np.mean(accepted)
        est_accept_rate = np.where(est_accept_rate!=0,est_accept_rate,1*10**(-6) )    
        est_accept_rate = np.where(est_accept_rate!=1,est_accept_rate,1-1*10**(-6) )
        #est_accept_rate[[est_accept_rate == 0]] = 1*10**(-6)
        #est_accept_rate[est_accept_rate == 1] = 1 - 1*10**(-6)
        
        # Calculate the expected number of MCMC steps required from the
        # estimated acceptance rate. The number of steps cannot go
        # above a user-specified value
        R = np.ceil( np.log(0.05) / np.log(1 - est_accept_rate) )
        R = np.min([R,max_MCMC_steps])
        print('MCMC steps: %d'%(R))
        # Perform the remaining MCMC steps
        
        #MVN_loop_set = partial(MVN_move, N_moves=int(R-1), part_thetas=part_thetas, part_outputs=part_outputs, part_summaries=part_summaries, part_Ds=part_Ds, Ctheta=Ctheta, theta_mins=theta_mins, theta_maxs=theta_maxs, scale_param=scale_param, target_D=target_D, Pt=Pt, Ot=Ot, f_simulate=f_simulate, f_summaries=f_summaries, f_discrep=f_discrep, seed_nums=seed_nums)
   
        #print('Accept rate: %g'%(est_accept_rate))
        #print('Running remaining MVN loops %d to %d '%(worst_keep,N_parts))
        #print('Creating %d particles'%(int(R-1)))
        #pool = get_context("spawn").Pool()
        
        iterable = []
        for k in range(worst_keep,N_parts):
            iterable.append((k,int(R-1), np.array(part_thetas[k,:]).ravel(), np.squeeze(part_outputs[k,:,:]), np.squeeze(part_summaries[k,:]), np.squeeze(part_Ds[k]), Ctheta, theta_mins, theta_maxs, scale_param, target_D, np.squeeze(Pt[seed_nums[k],:,:]), np.squeeze(Ot[seed_nums[k],:,:]), f_simulate, f_summaries, f_discrep))

        #print('Running first MVN loop %d to %d'%(worst_keep,N_parts))
        #pool = get_context("spawn").Pool()
        #pbar = tqdm(total=N_parts)
        print('mvn2...')
        start = time.time()
        #results = pool.map(MVN_loop_set, range(worst_keep,N_parts))
        #for i,k, part_theta, part_output, part_summary, part_D, accept  in tqdm(enumerate(pool.imap_unordered(MVN_loop_set,range(worst_keep,N_parts)))): 
        chunksize = int((N_parts-worst_keep)/process_count)
        if chunksize<1:
            chunksize=1
        results = pool.starmap_async(MVN_move,iterable,chunksize=chunksize)
        for r in results.get():
            k = r[0]
            part_thetas[k,:] = r[1]
            part_outputs[k,:,:] = r[2]
            part_summaries[k,:] = r[3]
            part_Ds[k] = r[4]
            accepted[k] = r[5]
        del iterable
        # print('Complete\n')
        #print(part_summaries)
        print('mvn2 duration: '+str(time.time()-start)) 
        # Count the number of unique particles 
        #unique_thetas = np.unique(part_thetas, axis=0)
        N_unique = np.shape(np.unique(part_thetas,axis=0))[0]
        
        # Output information on discrepancy and uniqueness if flag is set
        if verbose:
            print("Current discrepancy target is %g, number of unique particles is %g" % (target_D, N_unique))
        
        # Check to see if the desired discrepancy target has been reached, and
        # terminate the loop if so. The structure of the loop means that a
        # mutation step will happen after the final resample, so we expect a
        # full sample of N_parts particles all satisfying the desired
        # discrepancy constraint
        print("Discrepency differnce: %.2f\n" % (desired_D-target_D,))
        if target_D < desired_D:
            #print(""%(desired_D,))
            looping = 0
        
        #Also check to see if degeneracy is occurring - this happens when the
        # MCMC steps fail to find new locations and thus remain copies of
        # previous particles. This also terminates the loop when it becomes too
        # severe, but with additional output of a warning
        #print("Degeneracy differnce: %.2f\n" % (N_unique-N_parts))
    
        if N_unique <= (N_parts / 2):
            print("WARNING: SMC loop terminated due to particle degeneracy. Target not reached! \n")
            print("Best discrepancy: %.2f, Worst discrepancy: %.2f"%(part_Ds[0], part_Ds[worst_keep]))
            looping = 0;
        
        '''
        # Visualise Particles if flag is set
        if visualise
            
            % Call provided plotting function
            f_visualise(part_thetas, part_outputs, part_summaries, part_Ds);
            drawnow;
            
        end
        '''
        mem_used = psutil.virtual_memory()[2]
        print('memory % used:', mem_used)
        if mem_used > 90:
            print('WARNING: High memory usage.  Aborting!\n')
            looping = 0
        if time.time()>max_runtime:
            #print('WARNING: Runtime exceeded.  Aborting!')
            looping = 0
    # Close the parallel pool now that it's use is finished
    pool.close()
    pool.join()
    del pool
    print('SMCABC COMPLETE\n')
    return part_thetas, part_outputs, part_summaries, part_Ds


if __name__=='__main__':
    pass
