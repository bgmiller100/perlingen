# -*- coding: utf-8 -*-
"""
@author: Brodie Lawson et al. (see README), Benjamin Miller 
 Benjamin Miller

Operating interface for launching Perlin noise image generation and Monte-Carlo Bayesian estimation algorithms per [1].  Parses variables, assigns subfunctions, and controls image output.

IN: Call -h for detailed input options. Includes noise state variables, number of images, number of SMC-ABC particles, tolerable discrepancy, runtime limit, and boolean to run the SMC algorithm after image generation.

OUT: Input and estimated images as png files.  Text file with corresponding state variables.  Also returns metric sets and discrepancies of each result, not presently incorporated.  

[1](Jakes, David & Burrage, Kevin & Drovandi, Christopher & Burrage, Pamela & Bueno-Orovio, Alfonso & Santos, Rodrigo & Rodriguez, Blanca & Lawson, Brodie. (2019). Perlin Noise Generation of Physiologically Realistic Patterns of Fibrosis. 10.1101/668848.)

This function performs SMC-ABC with resampling by "an independent Metropolis-Hastings kernel" as laid out by:
Drovandi, C. C. and Pettitt, A. N. (2011). Estimation of Parameters for
Macroparasite Population Evolution using Approximate Bayesian Computation.
Biometrics, 67(1):225-233.

Sets of particles satisfying a set of intermediary "discrepancy cutoffs"
are generated by successive resampling and mutation steps in the vein of
traditional SMC. The function is designed to be modular, so accepts as
input the functions that will be used to simulate the model, and to
calculate the discrepancy with the target data. Further options may be
specified by the user within the body of the program.

Usage:   [part_thetas, part_vals] = performSMCABC(N_parts, f_simulate, f_summaries, f_discrep, target_data, params_mins, params_maxs, scale_param, desired_D, visualise, (f_visualise), (options) )
        print('worst tempering ESS possible: %.4f'%(ESS[np.argmax(np.abs(ESStarget-ESS))]))

INPUTS
-------
N_parts - The number of particles to generate. Corresponds to the
          expected number of outputs satisfying the discrepancy criteria
          at the end of the method

f_simulate - The function used to simulate the model. This should
             generate only one output (but a struct could be used to
             store varied information/multiple outputs). The input is a
             single vector of parameters

f_summaries - The function used to calculate the summary statistics for
              given data. Takes as input a single object, which is the
              output of f_simulate. The target data should also be
              specified in this format

f_discrepancy - The function used to calculate the discrepancy between
                two sets of metrics. As a basis this would be the
               Euclidean or Mahalanobis distance, but is supplied
                separately so that extra problem-specific modifications
                can be made (e.g. taking into account periodicity of
                angular measures)

target_data - The data, of equivalent type to that generated by
              f_simulate, that the SMC_ABC routine seeks to generate
             samples sufficiently close to

params_mins - The minimum values allowed for the parameters of the model

params_maxs - The maximum values allowed for the parameters of the model

scale_param - Vector of logicals of length equal to number of parameters.
              Specifies which parameters are 'scale' parameters that are
              to be transformed logarithmically so as to have a uniform
              prior.

desired_D - The discrepancy that the algorithm will attempt to achieve
            (typical D values depend on application, so this is
            externally specified by the user)

NOT IMPLEMENTED:
    visualise - A boolean flag specifying if the user wants to visualise the
                output during the course of the algorithm

    (f_visualise) - A function (supplied by user because likely
                    application-specific) that visualises the results.
                    Optional argument, only used if visualise flag is set.
                    Syntax must be
                    visFunc( params, outputs, summaries, discrepancies )

    (options) - A struct containing modifications to the SMC-ABC algorithm's
                default options/parameters. Information regarding these is
                contained within the file. User may set:

                options.resample_weighting ('equal', 'weighted')
                options.metric_weighting ('mahalanobis', 'variance', 'none')
                options.keep_fraction ([0,1])
                options.max_MCMC_steps (positive integer)
                options.verbose (0 - false, 1 - true)

                For optional outputs, (options) must come last (but can be supplied after
                'visualise' flag if it is set to zero)

OUTPUTS
--------

part_thetas - The locations in parameter space of the final particles

part_vals - The model outputs corresponding to each particle
"""
import time
import numpy as np
def runSim(k, part_params, Pt, Ot, f_simulate, f_summaries): #seed_nums
    part_output = np.asarray(f_simulate(part_params, Pt, Ot),dtype='f8')
    part_summary = np.asarray(f_summaries(part_output),dtype='f8')
    return k, part_output, part_summary

def MVN_move(k, N_moves, theta, output, summaries, D, Ctheta, theta_mins, theta_maxs, scale_param, target_D, Pt_pass, Ot_pass, f_simulate, f_summaries, f_discrep, burnin=0):
    # This function performs MCMC updates ('move steps' or 'mutations') for a
    # single particle, using a multivariate normal jumping distribution
  
    # Initialise variable 'moved' as false
    moved = 0
    # Perform the requested number of move steps
    mvcount=0
    brokeout = 0
    burnpass = 0.08
    movespast = max(12, N_moves*burnpass)
    for y in range(N_moves):
        #disabled  burn in currently 
        if burnin==1 and mvcount>=movespast and moved==1: #will only reduce runs if given a burnin label
            brokeout = 1
            break #handle move to high-probability region with breakout speed-up
            
            #sudden, rapid degeneracy when reducing R to R/2
            #new attempt - break only after establishing a high probability position 
        # Pick a new location from the multivariate normal centred around the
        # requested point
        prop_theta = np.random.multivariate_normal(theta, Ctheta)
        
        # Instant rejection if particle moves outside boundary, so only process
        # if new proposed location is inside prior space
        if all(prop_theta <= theta_maxs) & all(prop_theta >= theta_mins):
            
            mydtype='f8'#'f8'
            # Convert scaling parameters back into true values for simulation
            prop_params = np.copy(prop_theta).astype(mydtype)
            prop_params[scale_param] = np.exp(prop_params[scale_param]);
            #print(prop_params[scale_param])
            # Calculate the associated model output, and discrepancy
            #time1=time.time()
            prop_output = np.asarray(f_simulate(prop_params, Pt_pass, Ot_pass),dtype=mydtype)
            #time2=time.time()
            prop_summaries = np.asarray(f_summaries(prop_output),dtype=mydtype)
            #time3=time.time()
            prop_D = np.asarray(f_discrep(prop_summaries),dtype=mydtype)
            #time4=time.time()
            #print('SimTime:%.4f, SumTime:%.4f, DisTime:%.4f'%(time2-time1,time3-time2,time4-time3))
            # Now accept this move if it still satisfies the current discrepancy
            # constraint (ratio of jumping distribution probabilities is one)
            if prop_D <= target_D:
                # Update all values for this particle
                theta[:] = prop_theta
                output[:] = prop_output
                D = np.copy(prop_D)
                summaries[:] = prop_summaries
                # Switch 'moved' flag to true
                moved = 1
                mvcount+=1
                #break #escape as soon as good, to hasten
    return k, theta, output, summaries, D, moved, brokeout

def warmup(x):
    x = x * x
    return x

def check_positive(value):
    ivalue = int(value)
    if ivalue <=0:
        raise argparse.ArgumentTypeError("%s is an invalid positive int value" % value)
    return ivalue
 
if __name__=='__main__':
    #from lib.File import Class
    from os import path
    import numpy as np
    import lib.lib_create.buildMesh as buildMesh
    import lib.lib_create.generateSeedData as generateSeedData
    import lib.lib_create.generatePatterns as generatePatterns
    import lib.lib_create.createFibroPattern as createFibroPattern
    import lib.lib_metrics.calculateMetrics as calculateMetrics
    import lib.lib_metrics.ellipseDiscrepancy as ellipseDiscrepancy
    import os
    from sys import exit
    import argparse
    from functools import partial
    import faulthandler
    import time
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt

    faulthandler.enable()
    print('Launching...')
    parser = argparse.ArgumentParser(description='Generate Perlin noise')

    parser.add_argument('--lb','--feature_size', type=float,default=.08,help='base obstacle size [.01,2] (.08)',metavar='LB')
    parser.add_argument('--g','--roughness', type=float,default=.1,help='base roughness [0, 0.99] (.1)',metavar='G')
    parser.add_argument('--r','--fibre_alignment', type=float,default=15,help='base anisotropy [.5,50] (15)',metavar='R')

    parser.add_argument('--ld','--patch_size', type=float,default=2,help='density field feature size [1,8] (2)',metavar='LD')

    parser.add_argument('--l','--fibre_sep', type=float,default=.6,help='fibre seperation distance [.3,2] (.6)',metavar='L')
    parser.add_argument('--p','--direction', type=float,default=45,help='fiber orientation [-90,90] (45)',metavar='P')
    parser.add_argument('--fp','--fibre_period', type=float,default=2,help='fibre element widths [x,x] (2)',metavar='FP')

    parser.add_argument('--f','--fibreness', type=float,default=.4,help='mixing extent of fibre pattern [0,0.4] (.4)',metavar='F')
    parser.add_argument('--d','--patchiness', type=float,default=.05,help='mixing local density magnitude [0,.5] (.05)',metavar='D')
    parser.add_argument('--c','--collagen', type=float,default=.2,help='mixing collagen density threshold [0,.99] (.2)',metavar='C')

    parser.add_argument('--num','--number_images',type=check_positive, default=1, help='number of images to generate (1)',metavar='NUM')
    parser.add_argument('--pw','--pixel_width', type=float,default=.0075,help='pixel width, zooming [.001,.05] (.0075)',metavar='PW')
    parser.add_argument('--fs','--n_fibres_similarity', type=float,default=4,help='keep fixed, the similarity off fibres [.5,5] (4)',metavar='FS')
    parser.add_argument('--wl','--wiggle_feature_length', type=float,default=4,help='keep fixed, the length between wave peaks [2,50] (4)',metavar='WL')
    parser.add_argument('--ps','--phasefield_strength', type=float,default=5,help='keep fixed, amplitude of fibre waves [.01,10] (5)',metavar='PS')


    parser.add_argument('--particles','--N_parts', type=check_positive,default=2000,help='number of particles for SMC-ABC [2000 recommended] (50)',metavar='PARTS')
    parser.add_argument('--discrepancy','--desired_D', type=float,default=.5,help='desired discrepancy of error metrics in estimation (.5)',metavar='DISCREP')
    parser.add_argument('--time','--max_time',type=check_positive, default=15*60, help='runtime limit of the smcabc algorithm in seconds, for safety (15*60)',metavar='TIME')
    parser.add_argument('--smcabc','--rumn_smcabc',type=check_positive, default=1, help='0/1 boolean to run the esimation algorithm after target image generation (1)',metavar='SMCABC')

    args = parser.parse_args();
    num_imgs = args.num 
    feature_size = args.lb
    roughness = args.g 
    fibre_alignment = args.r
    patch_size = args.ld 
    fibre_sep = args.l
    direction = args.p
    fibreness = args.f
    patchiness = args.d
    density = args.c
    pixel_width = args.pw
    n_fibres_similarity=args.fs
    wiggle_feature_length=args.wl
    phasefield_strength=args.ps
    fibre_period=args.fp
    N_parts = args.particles
    desired_D = args.discrepancy
    max_time=args.time
    run_smcabc=args.smcabc
    """	
    DICTIONARY (call python3 perlingen.py -h)
    #BASE NOISE (Ob fibrosis field) (impedes lines)
    #   lb = "feature_size" = base obstacle size [.01,2]
    #   gamma = "roughness" = base roughness FIXED[0, 0.99]
    #   r = "fibre_alignment =  base anisotropy [0.5,50]
    feature_size = .08; #lb
    roughness = 0.1; #gamma
    fibre_alignment = 15; #r
    #DENSITY VARIATION (Od density field)
    #   ld = "patch_size" density feature size [1,8]
    patch_size = 2; #ld
    #FIBRE SELECTION (F sinusoisal field)
    #   L = "fibre_sep" seperation distance [0.3,2]
    #   Phi = "direction"  = fiber orientation [-pi/2, pi/2]
    fibre_sep = 0.6; #L
    direction = 45*np.pi/180; #Phi
    fibre_sep = 0.6; #L
    direction = 45*np.pi/180; #Phi
    #MISC
    #   f = "fibreness" = extent of pattern of fibers[0,0.4]
    #   d = "patchiness" magnitude of local density [0, 0.5]
    #   collagen density threshold [0,.99]
    fibreness = .4; #f
    patchiness = .05; #d
    density = 0.2;
    """
    #IMAGEDATA
    N_patterns = num_imgs;
    Nx = 500;
    Ny = 500;
    #pixel_width = .0075;
    #SEEDDATA
    N_seeds = 500
    N_freqs = 8;
    seed = None;

    ## RUN
    #build mesh
    mesh = buildMesh.main(Nx,Ny,pixel_width); #(Nx,Ny,pixel_width);
    params = {'fibreness':fibreness, 'fibre_sep':fibre_sep, 'patchiness':patchiness,
              'feature_size':feature_size,'roughness':roughness, 'patch_size':patch_size,
              'fibre_alignment':fibre_alignment, 'direction':direction,
              'n_fibres_similarity':n_fibres_similarity, 
              'wiggle_feature_length':wiggle_feature_length, 
              'phasefield_strength':phasefield_strength,'fibre_period':fibre_period}

    #check for results directory
    try:
        os.makedirs('results')
    except OSError:
        if not os.path.isdir('results'):
                raise
    #run generator
    patterns, img = generatePatterns.main(params, density, N_patterns, mesh, num_imgs);
    target_data = img
    plt.imsave('results/input_target.png',target_data)
    np.save("results/generated_patterns.npy",patterns)

    #run smcabc
    if run_smcabc==1:
        f_simulate = partial(createFibroPattern.main, mesh, density)
        f_summaries = calculateMetrics.main
        f_discrepancy = ellipseDiscrepancy.main
        #params_mins = np.array([0.0, 0.3, 0.0, 0.01, 0.00, 1., 00.5, -90.])
        #params_maxs = np.array([0.4, 2.0, 0.5, 2.00, 0.99, 5., 20.0, 90.])
        #                                                   8   50
        #generated              [0.4, 0.6, 0.05,0.08, 0.10, 2., 15.0., 45]
        params_mins = np.array([0.0, 0.3, 0.00, 0.01, 0.00, 1., 00.5, -90])
        params_maxs = np.array([0.4, 1.0, 0.10, 0.50, 0.30, 5., 20.0,  90])
        scale_param = np.array([ 0,    0,    0,    0,    0, 0,     1,   0], dtype=int)

        print("running smcabc...\n")
        start_all = time.time()
        print('TOTAL RUNTIME: '+str(time.time()-start_all))
    else:
        print('Not running smcabc')
        exit()
    
    #import numpy as np
    import lhsmdu
    from multiprocessing import Pool, cpu_count, get_context
    from functools import partial
    import psutil
    from tqdm import tqdm
    from mpi4py.futures import MPIPoolExecutor
    
    '''
    %%%%%%%%%%%%%%%% VARIABLE INPUT %%%%%%%%%%%%%%%%%%%%%%%%%
    '''
    
    #Default options (can be changed here or adjusted on the fly by supplying options argument)
    #resample_weighting = 'weighted';  
    
    # 'equal' - All kept particles are weighted equally for resampling steps
    # 'weighted' - Particles are weighted according to their discrepancy values when resampling (taking
    #              worst kept particle as three sigma in a normal distribution, and then normalising
    #              weights so they sum to one)
    
    #metric_weighting = 'variance';  
    # 'mahalanobis' - Mahalanobis distance is used for calculations of discrepancy
    # 'variance' - Metrics are scaled according to their variance, but covariances are ignored
    # 'none' - Euclidean distance is used
    # (All options still include rescaling of angle metrics with respect to eccentricity)
    
    keep_fraction = 0.75 #0.75; 0.50 for high-efficacy          
    # The proportion of particles to keep during each resample step (recommended ~0.5 as a starting value)
    if N_parts<100:
        max_MCMC_steps = 50
    else:
        max_MCMC_steps = 300
    # The maximum number of 'jiggle' steps applied to all particles in the attempt to find unique locations
    verbose = 1;                 
    #Output information about particle uniqueness and discrepancy targets
    
    
    '''
    %%%%%%%%%%%%%%%% INSTANTIATE%%%%%%%%%%%%%%%%%%%%%%%%%
    '''
    
	
    process_count = cpu_count()  
    print('cpus: %d / %d'%(process_count,cpu_count()))
    max_runtime = time.time()+max_time
    print('beginning pool...')
    start = time.time()
    ESStarget=0.8
   
    burnidx = 50 #disabled
    #5 #number of iterations to burn-in (use full R)

    #generate and warm up pool 
    
    #executor = MPIPoolExecutor(10)
    #pool = get_context("spawn").Pool(processes = process_count, maxtasksperchild=1)
    time.sleep(1)
    end = time.time()
    print('time elapsed: '+str(end-start))
    print('warming up...')
    start = time.time()
    
    with MPIPoolExecutor() as executor:
        res = list(executor.starmap(warmup,zip(range(process_count))))
        print(res)
    #pool.map(warmup, range(process_count), chunksize=1)
    time.sleep(1)
    end  = time.time()
    print('time elapsed: '+str(end-start))
    print('parent precent memory used: '+str(psutil.virtual_memory()[2]))
    executor.shutdown()
    exit()


    # Calculate the summaries for the target data
    print('calculating target summaries...')
    start=time.time()
    target_summaries = np.asarray(f_summaries(target_data),dtype='f8')
    end=time.time()
    #print('calculated, time elapsed: '+str(end-start))
    # Calculate the ordinal location of the last particle kept for convenience
    worst_keep = int(np.rint(N_parts * keep_fraction)+1)
    # Read out the number of variables
    N_theta = len(params_mins)
    
    # Convert scaling parameters to log equivalents. The set of transformed
    # parameters is termed 'theta' in this code
    #print('scaling...')
    scale_param = scale_param==1
    theta_mins = np.copy(params_mins).astype('f8')
    theta_mins[scale_param] = np.log(theta_mins[scale_param])
    theta_maxs = np.copy(params_maxs).astype('f8')
    theta_maxs[scale_param] = np.log(theta_maxs[scale_param])
   
    # Initialise particles using Latin Hypercube Sampling
    #print('generating lhsmdu...')
    part_lhs = lhsmdu.sample(N_parts, N_theta)    
    part_thetas = theta_mins + np.multiply(part_lhs,(theta_maxs - theta_mins))
    
    # For simulation, convert scaling parameters back to true values
    part_params = np.copy(part_thetas).astype('f8')
    part_params[:,scale_param] = np.exp(part_params[:,scale_param])

    # Load seed data (generated by a separate file)
    #print('loading seed data...')
    seedinfo = np.load("seedinfo.npy",allow_pickle=True)
    Pt = np.asarray(seedinfo.item().get('permute_tables'),dtype='f8')
    Ot = np.asarray(seedinfo.item().get('offset_tables'),dtype='f8')
    #Count number of seeds created
    N_seeds = np.shape(Pt)[0]
    
    '''
    %%%%%%%%%%%%%%%% PARPOOL 1: FIRST MCMC STEP %%%%%%%%%%%%%%%%%%%%%%%%%
    '''

    # Generate the seed information for each particle, then simulate the model
    # using this seed information, storing both its output and the associated
    # summary statistics    
    
    #Create an individual simulator object for this praticle, using the
    # seed information
    print('creating savespace...')
    seed_nums = np.random.choice(a=range(N_seeds),size=N_parts)
    part_outputs = np.zeros((N_parts, mesh['Nx'], mesh['Ny'])).astype('f8')
    part_summaries = np.zeros((N_parts, 27)).astype('f8')

    # Run the simulator and store summaries
    time.sleep(5)
    print('creating partial...')
    #runSimLoop  = partial(runSim, part_params=part_params, Pt=Pt, Ot=Ot, f_simulate=f_simulate, f_summaries=f_summaries, seed_nums=seed_nums)

    iterable = []
    for k in range(N_parts):
        iterable.append((k,part_params[k,:], Pt[seed_nums[k],:,:], Ot[seed_nums[k],:,:], f_simulate, f_summaries))
    time.sleep(5)
    #runSim_lambda = lambda k: runSim(k,part_params[k,:],Pt[seed_nums[k],:,:],Ot[seed_nums[k],:,:],f_simulate,f_summaries)
    print('parent precent memory used: '+str(psutil.virtual_memory()[2]))

    print('beginning map...')
    start_loop = time.time()
    #results = pool.map(runSimLoop,range(N_parts), chunksize=1)
    #results = []
    #for i,k,part_out, part_sum in tqdm(enumerate(pool.imap_unordered(runSimLoop,range(N_parts),chunksize=int(N_parts/process_count)))):
        #results.append(res)
    chunksize = int(N_parts/process_count)
    if chunksize<1:
        chunksize=1
    #results = pool.starmap_async(runSim,iterable,chunksize=chunksize)
    #results_get = results.get()
    #for r in results_get:
    for r in executor.starmap(runSim,iterable):
        k = r[0]
        part_outputs[k,:,:] = r[1]
        part_summaries[k,:]=r[2]
        #pbar.update()
    del iterable
    print('time elapsed: '+str(time.time()-start_loop))
    print('parent precent memory used: '+str(psutil.virtual_memory()[2])+'\n')
    '''
    %%%%%%%%%%%%%%%% WEIGHTING/RESAMPLING %%%%%%%%%%%%%%%%%%%%%%%%%
    '''
            
    # Calculate sample covariance matrix from the initial particles
    C_S = np.cov(np.transpose(part_summaries))
    # Use diagonals of this to extract only variances (no covariances)
    C_S = np.diag(np.diag(C_S))
    # Store the inverse, so that it only need be calculated once
    invC_S = np.linalg.inv(C_S)
    # Use discrepancy function with this inverse matrix included
    f_discrep = partial(f_discrepancy,target_metrics_old = target_summaries, invC = invC_S)
    # Calculate discrepancies for each particle based on this distance measure
    part_Ds = np.asarray(f_discrepancy(part_summaries, target_summaries, invC_S),dtype='f8')

    # Loop until stopping criteria is hit
    looping = 1
    testitcount = 1
    while looping: 
        print('Iteration count: %g' % (testitcount,))
        if testitcount > burnidx:
            burnin=1
        else:
            burnin=0

        # First, sort all the particles according to their discrepancy
        ordering = np.argsort(part_Ds) 
        part_Ds = part_Ds[ordering]
        part_outputs = part_outputs[ordering,:,:]
        part_thetas = part_thetas[ordering,:]
        part_summaries = part_summaries[ordering,:]
        
        # Now select the target discrepancy as the worst particle of the kept
        # fraction
        target_D = np.copy(part_Ds[worst_keep])
        
        # Select which particles, from those that are kept, to resample the
        # discarded particles onto.
        # Calculate weights for each particle. These are found by
        # assuming that the worst kept particle is three standard
        # deviations away from D = 0. Due to normalisation of weights,
        # which here takes place inside randsample, even if all
        # particles are far from D = 0 it will not be an issue.
        
        part_logweights = - 9 * (part_Ds[:worst_keep]**2) / part_Ds[worst_keep]**2
        part_logweights = part_logweights + part_logweights.max() 
  
        #Normalise so largest weight is 1 (0 on log scale)
        #part_weights = np.exp(part_logweights)
        #part_weights = part_weights / np.sum(part_weights)

        # Tempering and Normalizing
        #via https://arxiv.org/pdf/1805.03317.pdf section 2.2
        BetaSet = np.linspace(0, 1, num=100)
        part_weights = np.exp(part_logweights)
        weights = np.tile(part_weights, (len(BetaSet),1))
        weights = np.divide(np.power(weights,BetaSet[:,None]),np.sum(np.power(weights,BetaSet[:,None]), axis=1)[:,None])
        ESS = (1/np.sum(weights**2, axis=1))/(N_parts*keep_fraction)
        ESSind = np.argmin(np.abs(ESStarget-ESS))
        alpha = 20 #cooling rate for beta*(iteration/alpha)
        Beta = BetaSet[ESSind]*max(1,((testitcount-1)/alpha)) 
        part_weights = part_weights**Beta / np.sum(part_weights**Beta)
        print('Weight tempering exponent: %.3f'%(Beta))



        # Now select particles from the kept particles, according to
        # their weights, to decide which particles to copy ontio
        selection = np.random.choice(a=range(worst_keep), size=N_parts-worst_keep,replace=False,p=part_weights)
   
        # Now perform the particle copy
        part_thetas[worst_keep:,:] = part_thetas[selection,:]
        part_outputs[worst_keep:,:,:] = part_outputs[selection,:,:]
        part_summaries[worst_keep:,:] = part_summaries[selection,:]
        part_Ds[worst_keep:] = part_Ds[selection]
        accepted = np.zeros(np.shape(part_Ds))
        
        # Now the particles are 'jiggled' so that we return to having a set of
        # unique particles. This step can also be thought of as performing
        # local exploration, and is sometimes called 'mutation'. MCMC steps are
        # used to perform mutation, and an advantage of SMC approaches is that
        # the positions of all particles can be used to decide on a sensible
        # jumping distribution.
        
        # Create a weighted covariance matrix to use in a multivariate
        # normal jumping distribution. 2.38^2/N is the 'optimal' factor
        # under some assumptions
        Ctheta = (2.38**2 / N_theta) * np.cov(np.transpose(part_thetas))

        '''
        %%%%%%%%%%%%%%%% PARPOOL 2: LOOP MCMC STEP %%%%%%%%%%%%%%%%%%%%%%%%%
        '''
        
        #Create an individual simulator object for this praticle, using the    
    
        # Perform one MCMC step to evaluate how many are expected to be required
        
        #MVN_loop_set = partial(MVN_move, N_moves=1, part_thetas=part_thetas, part_outputs=part_outputs, part_summaries=part_summaries, part_Ds=part_Ds, Ctheta=Ctheta, theta_mins=theta_mins, theta_maxs=theta_maxs, scale_param=scale_param, target_D=target_D, Pt=Pt, Ot=Ot, f_simulate=f_simulate, f_summaries=f_summaries, f_discrep=f_discrep, seed_nums=seed_nums)
        iterable = []
        for k in range(worst_keep,N_parts):
            iterable.append((k,1,np.array(part_thetas[k,:]).ravel(), np.squeeze(part_outputs[k,:,:]), np.squeeze(part_summaries[k,:]), np.squeeze(part_Ds[k]), Ctheta,theta_mins, theta_maxs, scale_param, target_D, np.squeeze(Pt[seed_nums[k],:,:]), np.squeeze(Ot[seed_nums[k],:,:]), f_simulate, f_summaries, f_discrep, burnin))

        print('mvn1...')
        start = time.time()
        #for i,k, part_theta, part_output, part_summary, part_D, accept  in tqdm(enumerate(pool.imap_unordered(MVN_loop_set,range(worst_keep,N_parts)))): 
        chunksize=int((N_parts-worst_keep)/process_count)
        if chunksize<1:
            chunksize=1
        results = pool.starmap_async(MVN_move,iterable,chunksize=chunksize)
        for r in results.get():
            k = r[0]
            part_thetas[k,:] = r[1]
            part_outputs[k,:,:] = r[2]
            part_summaries[k,:] = r[3]
            part_Ds[k] = r[4]
            accepted[k] = r[5]
        del iterable
        print('mvn1 duration: '+str(time.time()-start)) 
        
        # Calculate the acceptance rate, and ensure that the case where
        # all or no particles are accepted does not result in a number
        # of MCMC steps that cannot be calculated
        est_accept_rate = np.mean(accepted)
        est_accept_rate = np.where(est_accept_rate!=0,est_accept_rate,1*10**(-6) )    
        est_accept_rate = np.where(est_accept_rate!=1,est_accept_rate,1-1*10**(-6) )
        
        # Calculate the expected number of MCMC steps required from the
        # estimated acceptance rate. The number of steps cannot go
        # above a user-specified value
        R = np.ceil( np.log(0.05) / np.log(1 - est_accept_rate) )
        R = np.min([R,max_MCMC_steps])
        print('MCMC steps: %d'%(R))
        # Perform the remaining MCMC steps
        
        #MVN_loop_set = partial(MVN_move, N_moves=int(R-1), part_thetas=part_thetas, part_outputs=part_outputs, part_summaries=part_summaries, part_Ds=part_Ds, Ctheta=Ctheta, theta_mins=theta_mins, theta_maxs=theta_maxs, scale_param=scale_param, target_D=target_D, Pt=Pt, Ot=Ot, f_simulate=f_simulate, f_summaries=f_summaries, f_discrep=f_discrep, seed_nums=seed_nums)
   
        #print('Accept rate: %g'%(est_accept_rate))
        #print('Running remaining MVN loops %d to %d '%(worst_keep,N_parts))
        #print('Creating %d particles'%(int(R-1)))
        
        iterable = []
        for k in range(worst_keep,N_parts):
            iterable.append((k,int(R-1), np.array(part_thetas[k,:]).ravel(), np.squeeze(part_outputs[k,:,:]), np.squeeze(part_summaries[k,:]), np.squeeze(part_Ds[k]), Ctheta, theta_mins, theta_maxs, scale_param, target_D, np.squeeze(Pt[seed_nums[k],:,:]), np.squeeze(Ot[seed_nums[k],:,:]), f_simulate, f_summaries, f_discrep,burnin))

        print('mvn2...')
        start = time.time()
        #for i,k, part_theta, part_output, part_summary, part_D, accept  in tqdm(enumerate(pool.imap_unordered(MVN_loop_set,range(worst_keep,N_parts)))): 
        chunksize = int((N_parts-worst_keep)/process_count)
        if chunksize<1:
            chunksize=1
        brokeout=0
        results = pool.starmap_async(MVN_move,iterable,chunksize=chunksize)
        for r in results.get():
            k = r[0]
            part_thetas[k,:] = r[1]
            part_outputs[k,:,:] = r[2]
            part_summaries[k,:] = r[3]
            part_Ds[k] = r[4]
            accepted[k] = r[5]
            brokeout += r[6]
        del iterable
        print('mvn2 duration: '+str(time.time()-start)) 
        brokepercent = 100*brokeout / (N_parts-worst_keep)
        print('Breakout = %.2f %%'%(brokepercent))
        # Count the number of unique particles 
        #unique_thetas = np.unique(part_thetas, axis=0)
        N_unique = np.shape(np.unique(part_thetas,axis=0))[0]
        
        # Output information on discrepancy and uniqueness if flag is set
        if verbose:
            print("Current discrepancy target is %g, number of unique particles is %g" % (target_D, N_unique))
        
        # Check to see if the desired discrepancy target has been reached, and
        # terminate the loop if so. The structure of the loop means that a
        # mutation step will happen after the final resample, so we expect a
        # full sample of N_parts particles all satisfying the desired
        # discrepancy constraint
        print("Discrepency differnce: %.2f\n" % (desired_D-target_D,))
        if target_D < desired_D:
            looping = 0
        
        #Also check to see if degeneracy is occurring - this happens when the
        # MCMC steps fail to find new locations and thus remain copies of
        # previous particles. This also terminates the loop when it becomes too
        # severe, but with additional output of a warning
        #print("Degeneracy differnce: %.2f\n" % (N_unique-N_parts))
    
        if N_unique <= (N_parts / 2):
            print("WARNING: SMC loop terminated due to particle degeneracy. Target not reached! \n")
            looping = 0;
        
        '''
        # Visualise Particles if flag is set
        if visualise
            
            % Call provided plotting function
            f_visualise(part_thetas, part_outputs, part_summaries, part_Ds);
            drawnow;
            
        end
        '''
        '''
        #Memory and runtime safety features 
        mem_used = psutil.virtual_memory()[2]
        print('memory % used:', mem_used)
        if mem_used > 90:
            print('WARNING: High memory usage.  Aborting!\n')
            looping = 0
        if time.time()>max_runtime:
            #print('WARNING: Runtime exceeded.  Aborting!')
            looping = 0
        testitcount+=1
        '''
        break
    # Close the parallel pool now that it's use is finished
    pool.close()
    pool.join()
    del pool

    #Sort final unique output
    ordering = np.argsort(part_Ds) 
    part_thetas = np.copy(np.unique(part_thetas[ordering,:], axis=0))
    _,idx = np.unique(part_thetas,axis=0,return_index=True)
    
    part_thetas = np.copy(part_thetas[idx,:])
    part_Ds = np.copy(part_Ds[idx])
    part_outputs = np.copy(np.asarray(part_outputs[idx,:,:],dtype='f8'))
    part_summaries = np.copy(part_summaries[idx,:])

    print("Best discrepancy: %.2f, Worst discrepancy: %.2f"%(part_Ds[0], part_Ds[-1]))

    print('SMCABC COMPLETE\n')
    print('TOTAL SMCABC RUNTIME: '+str(time.time()-start_all))
    
    print('printing unique results')
    for i in range(np.shape(part_outputs)[0]):
        outfile = 'results/out_%05d.png' % (i)
        plt.imsave(outfile,np.squeeze(part_outputs[i,:,:]))
    print('...\n')
    '''
    print('input: f %.3f,  l %.3f, d %.3f, lb %.3f, g %.3f, ld %.3f, r %.3f, p %.3f'%(params['fibreness'], params['fibre_sep'], params['patchiness'], params['feature_size'], params['roughness'], params['patch_size'], params['fibre_alignment'], params['direction']))

    #save raw data
    np.save("results/part_thetas.npy",part_thetas)
    np.save("results/part_outputs.npy",part_outputs)
    np.save("results/part_summaries",part_summaries)
    np.save("results/part_Ds.npy",part_Ds)

    print('...\n')

    for i in range(np.shape(part_thetas)[0]):
        print('out_%05d: f %.3f,  l %.3f, d %.3f, lb %.3f, g %.3f, ld %.3f, r %.3f, p %.3f'%(i, part_thetas[i,0], part_thetas[i,1], part_thetas[i,2], part_thetas[i,3], part_thetas[i,4], part_thetas[i,5], part_thetas[i,6], part_thetas[i,7]))
    print('...\n')
    '''
